{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "class BaseEmbeddings:\n",
    "    \"\"\"\n",
    "    Base class for embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, path:str, is_api:bool) -> None:\n",
    "        self.path = path\n",
    "        self.is_api = is_api\n",
    "\n",
    "    def get_embedding(self, text:str, model:str) -> List[float]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @classmethod\n",
    "    def cosine_similarity(cls, vectors1:List[float], vectors2:List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two vectors\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(vectors1, vectors2)\n",
    "        magnitude = np.linalg.norm(vectors1) * np.linalg.norm(vectors2)\n",
    "        if not magnitude:\n",
    "            return 0\n",
    "        return dot_product / magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIEmbeddings(BaseEmbeddings):\n",
    "    \"\"\"\n",
    "    class for OpenAI embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, path:str='', is_api:bool=True) -> None:\n",
    "        super().__init__(path, is_api)\n",
    "        if self.is_api:\n",
    "            from openai import OpenAI\n",
    "            self.client = OpenAI()\n",
    "            self.client.api_key = os.getenv('OPENAI_API_KEY')\n",
    "            self.client.base_url = os.getenv('OPENAI_BASE_URL')\n",
    "    \n",
    "    def get_embedding(self, text: str, model: str = \"text-embedding-3-large\") -> List[float]:\n",
    "        if self.is_api:\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            return self.client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "        else:\n",
    "            raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class JinaEmbedding(BaseEmbeddings):\n",
    "    \"\"\"\n",
    "    class for Jina embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str = 'jinaai/jina-embeddings-v2-base-zh', is_api: bool = False) -> None:\n",
    "        super().__init__(path, is_api)\n",
    "        self._model = self.load_model()\n",
    "        \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        return self._model.encode([text])[0].tolist()\n",
    "    \n",
    "    def load_model(self):\n",
    "        import torch\n",
    "        from transformers import AutoModel\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        model = AutoModel.from_pretrained(self.path, trust_remote_code=True).to(device)\n",
    "        return model\n",
    "\n",
    "class ZhipuEmbedding(BaseEmbeddings):\n",
    "    \"\"\"\n",
    "    class for Zhipu embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, path:str='', is_api:bool=True):\n",
    "        super().__init__(path, is_api)\n",
    "        if self.is_api:\n",
    "            from zhipuai import ZhipuAI\n",
    "            self.client = ZhipuAI(api_key = os.getenv(\"ZHIPUAI_API_KEY\"))\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"embedding-2\",\n",
    "            input = text,\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "\n",
    "class DashscopeEmbedding(BaseEmbeddings):\n",
    "    \"\"\"\n",
    "    class for Dashscope embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str = '', is_api: bool = True) -> None:\n",
    "        super().__init__(path, is_api)\n",
    "        if self.is_api:\n",
    "            import dashscope\n",
    "            dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "            self.client = dashscope.TextEmbedding\n",
    "\n",
    "    def get_embedding(self, text: str, model: str='text-embedding-v1') -> List[float]:\n",
    "        response = self.client.call(\n",
    "            model=model,\n",
    "            input=text\n",
    "        )\n",
    "        return response.output['embeddings'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.文档加载及切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2\n",
    "# !pip install html2text\n",
    "# !pip install pymupdf\n",
    "# !pip install \"unstructured[md]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import markdown\n",
    "import html2text\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders.markdown import UnstructuredMarkdownLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "class ReadFiles:\n",
    "    \"\"\"\n",
    "    class to read files\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str) -> None:\n",
    "        self._path = path\n",
    "        self.file_list = self.get_files()\n",
    "\n",
    "    def get_files(self):\n",
    "        # args：dir_path，目标文件夹路径\n",
    "        file_list = []\n",
    "        for filepath, dirnames, filenames in os.walk(self._path):\n",
    "            # os.walk 函数将递归遍历指定文件夹\n",
    "            for filename in filenames:\n",
    "                # 通过后缀名判断文件类型是否满足要求\n",
    "                if filename.endswith(\".md\"):\n",
    "                    # 如果满足要求，将其绝对路径加入到结果列表\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "                elif filename.endswith(\".txt\"):\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "                elif filename.endswith(\".pdf\"):\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "        return file_list\n",
    "\n",
    "    def get_content(self, max_token_len: int = 600, cover_content: int = 150):\n",
    "        print(f\"该 目录下的文件数 {len(self.file_list)} ，文件列表为{self.file_list}\")\n",
    "        docs = []\n",
    "        # 读取文件内容\n",
    "        for file in self.file_list:\n",
    "            content = self.read_file_content(file)\n",
    "            print(f\"该 PDF 一共包含 {len(content)} 页\")\n",
    "            chunk_content = self.get_chunk(\n",
    "                content, max_token_len=max_token_len, cover_content=cover_content)\n",
    "            print(f'chunk_content类型：{type(chunk_content)}')\n",
    "            docs.extend(chunk_content)\n",
    "            # docs.append(chunk_content)\n",
    "        return docs\n",
    "\n",
    "    @classmethod\n",
    "    def get_chunk(cls, text: str, max_token_len: int = 600, cover_content: int = 150):\n",
    "        chunk_text = []\n",
    "\n",
    "        curr_len = 0\n",
    "        curr_chunk = ''\n",
    "\n",
    "        token_len = max_token_len - cover_content\n",
    "        lines = text.splitlines()  # 假设以换行符分割文本为行\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.replace(' ', '')\n",
    "            line_len = len(enc.encode(line))\n",
    "            if line_len > max_token_len:\n",
    "                # 如果单行长度就超过限制，则将其分割成多个块\n",
    "                num_chunks = (line_len + token_len - 1) // token_len\n",
    "                for i in range(num_chunks):\n",
    "                    start = i * token_len\n",
    "                    end = start + token_len\n",
    "                    # 避免跨单词分割\n",
    "                    while not line[start:end].rstrip().isspace():\n",
    "                        start += 1\n",
    "                        end += 1\n",
    "                        if start >= line_len:\n",
    "                            break\n",
    "                    curr_chunk = curr_chunk[-cover_content:] + line[start:end]\n",
    "                    chunk_text.append(curr_chunk)\n",
    "                # 处理最后一个块\n",
    "                start = (num_chunks - 1) * token_len\n",
    "                curr_chunk = curr_chunk[-cover_content:] + line[start:end]\n",
    "                chunk_text.append(curr_chunk)\n",
    "                \n",
    "            if curr_len + line_len <= token_len:\n",
    "                curr_chunk += line\n",
    "                curr_chunk += '\\n'\n",
    "                curr_len += line_len\n",
    "                curr_len += 1\n",
    "            else:\n",
    "                chunk_text.append(curr_chunk)\n",
    "                curr_chunk = curr_chunk[-cover_content:]+line\n",
    "                curr_len = line_len + cover_content\n",
    "\n",
    "        if curr_chunk:\n",
    "            chunk_text.append(curr_chunk)\n",
    "\n",
    "        return chunk_text\n",
    "\n",
    "    @classmethod\n",
    "    def read_file_content(cls, file_path: str):\n",
    "        # 根据文件扩展名选择读取方法\n",
    "        if file_path.endswith('.pdf'):\n",
    "            return cls.read_pdf(file_path)\n",
    "        elif file_path.endswith('.md'):\n",
    "            return cls.read_markdown(file_path)\n",
    "        elif file_path.endswith('.txt'):\n",
    "            return cls.read_text(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    # @classmethod\n",
    "    # def read_pdf(cls, file_path: str):\n",
    "    #     # 读取PDF文件\n",
    "    #     with open(file_path, 'rb') as file:\n",
    "    #         reader = PyPDF2.PdfReader(file)\n",
    "    #         text = \"\"\n",
    "    #         for page_num in range(len(reader.pages)):\n",
    "    #             text += reader.pages[page_num].extract_text()\n",
    "    #         return text\n",
    "    @classmethod\n",
    "    def read_pdf(cls, file_path: str):\n",
    "        # 读取PDF文件\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "        pdf_pages = loader.load()\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_pages)):\n",
    "            pdf_page = pdf_pages[page_num]\n",
    "            # 处理每一页的文本\n",
    "            import re\n",
    "            pattern = re.compile(r'[^\\u4e00-\\u9fff](\\n)[^\\u4e00-\\u9fff]', re.DOTALL)\n",
    "            pdf_page.page_content = re.sub(pattern, lambda match: match.group(0).replace('\\n', ''), pdf_page.page_content)\n",
    "            pdf_page.page_content = pdf_page.page_content.replace('•', '')\n",
    "            pdf_page.page_content = pdf_page.page_content.replace(' ', '')\n",
    "            text += pdf_page.page_content\n",
    "        return text\n",
    "    \n",
    "\n",
    "\n",
    "    # @classmethod\n",
    "    # def read_markdown(cls, file_path: str):\n",
    "    #     # 读取Markdown文件\n",
    "    #     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    #         md_text = file.read()\n",
    "    #         html_text = markdown.markdown(md_text)\n",
    "    #         # 使用BeautifulSoup从HTML中提取纯文本\n",
    "    #         soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    #         plain_text = soup.get_text()\n",
    "    #         # 使用正则表达式移除网址链接\n",
    "    #         text = re.sub(r'http\\S+', '', plain_text) \n",
    "    #         return text\n",
    "    \n",
    "    @classmethod\n",
    "    def read_markdown(cls, file_path: str):\n",
    "        # 读取Markdown文件\n",
    "        loader = UnstructuredMarkdownLoader(file_path)\n",
    "        md_pages = loader.load()\n",
    "        text = \"\"\n",
    "        for page_num in range(len(md_pages)):\n",
    "            md_page = md_pages[page_num]\n",
    "            md_page.page_content = md_page.page_content.replace('\\n\\n', '\\n')\n",
    "            text += md_page.page_content\n",
    "        return text\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def read_text(cls, file_path: str):\n",
    "        # 读取文本文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "\n",
    "class Documents:\n",
    "    \"\"\"\n",
    "        获取已分好类的json格式文档\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str = '') -> None:\n",
    "        self.path = path\n",
    "    \n",
    "    def get_content(self):\n",
    "        with open(self.path, mode='r', encoding='utf-8') as f:\n",
    "            content = json.load(f)\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.数据库 && 向量检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict,List, Optional, Tuple, Union\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, document:List[str]=['']) -> None:\n",
    "        self.document = document\n",
    "    \n",
    "    def get_vector(self, EmbeddingModel: BaseEmbeddings) -> List[List[float]]:\n",
    "        # 获得文档的向量表示\n",
    "        self.vectors=[]\n",
    "        for doc in tqdm(self.document, desc=\"Calculating embeddings\"):\n",
    "            self.vectors.append(EmbeddingModel.get_embedding(doc))\n",
    "            return self.vectors\n",
    "\n",
    "    def persist(self, path: str = 'storage'):\n",
    "        # 数据库持久化，本地保存\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        with open(f\"{path}/document.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.document, f, ensure_ascii=False)\n",
    "        if self.vectors:\n",
    "            with open(f\"{path}/vectors.json\", 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.vectors, f)\n",
    "\n",
    "    def load_vector(self, path: str = 'storage'):\n",
    "        with open(f\"{path}/vectors.json\", 'r', encoding='utf-8') as f:\n",
    "            self.vectors = json.load(f)\n",
    "        with open(f\"{path}/document.json\", 'r', encoding='utf-8') as f:\n",
    "            self.document = json.load(f)\n",
    "\n",
    "    def query(self, query: str, EmbeddingModel: BaseEmbeddings, k: int = 1) -> List[str]:\n",
    "        # 根据问题检索相关的文档片段\n",
    "        query_vector = EmbeddingModel.get_embedding(query)\n",
    "        result = np.array([self.get_similarity(query_vector, vector) for vector in self.vectors])\n",
    "        return np.array(self.document)[result.argsort()[-k:][::-1]].tolist()\n",
    "\n",
    "    def get_similarity(self,vector1:List[float], vector2:List[float]) -> float:\n",
    "        return BaseEmbeddings.cosine_similarity(vector1, vector2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.大模型模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = dict(\n",
    "    RAG_PROMPT_TEMPALTE=\"\"\"使用以上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。\n",
    "        问题: {question}\n",
    "        可参考的上下文：\n",
    "        ···\n",
    "        {context}\n",
    "        ···\n",
    "        如果给定的上下文无法让你做出回答，请回答数据库中没有这个内容，你不知道。\n",
    "        有用的回答:\"\"\",\n",
    "    InternLM_PROMPT_TEMPALTE=\"\"\"先对上下文进行内容总结,再使用上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。\n",
    "        问题: {question}\n",
    "        可参考的上下文：\n",
    "        ···\n",
    "        {context}\n",
    "        ···\n",
    "        如果给定的上下文无法让你做出回答，请回答数据库中没有这个内容，你不知道。\n",
    "        有用的回答:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def __init__(self, path: str = '') -> None:\n",
    "        self.path = path\n",
    "\n",
    "    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n",
    "        pass\n",
    "\n",
    "    def load_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIChat(BaseModel):\n",
    "    def __init__(self, path: str = '', model: str = \"gpt-3.5-turbo-1106\") -> None:\n",
    "        super().__init__(path)\n",
    "        self.model = model\n",
    "\n",
    "    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        client.api_key = os.getenv(\"OPENAI_API_KEY\")   \n",
    "        client.base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=history,\n",
    "            max_tokens=150,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternLMChat(BaseModel):\n",
    "    def __init__(self, path: str = '') -> None:\n",
    "        super().__init__(path)\n",
    "        self.load_model()\n",
    "\n",
    "    def chat(self, prompt: str, history: List = [], content: str='') -> str:\n",
    "        prompt = PROMPT_TEMPLATE['InternLM_PROMPT_TEMPALTE'].format(question=prompt, context=content)\n",
    "        response, history = self.model.chat(self.tokenizer, prompt, history)\n",
    "        return response\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        import torch\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.path, trust_remote_code=True)\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.path, torch_dtype=torch.float16, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DashscopeChat(BaseModel):\n",
    "    def __init__(self, path: str = '', model: str = \"qwen-turbo\") -> None:\n",
    "        super().__init__(path)\n",
    "        self.model = model\n",
    "\n",
    "    def chat(self, prompt: str, history: List[Dict], content: str) -> str:\n",
    "        import dashscope\n",
    "        dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n",
    "        response = dashscope.Generation.call(\n",
    "            model=self.model,\n",
    "            messages=history,\n",
    "            result_format='message',\n",
    "            max_tokens=150,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.output.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZhipuChat(BaseModel):\n",
    "    def __init__(self, path: str = '', model: str = \"glm-4\") -> None:\n",
    "        super().__init__(path)\n",
    "        from zhipuai import ZhipuAI\n",
    "        self.client = ZhipuAI(api_key=os.getenv(\"ZHIPUAI_API_KEY\"))\n",
    "        self.model = model\n",
    "\n",
    "    def chat(self, prompt: str, history: List[Dict], content: str) -> str:\n",
    "        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=history,\n",
    "            max_tokens=150,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Tiny-RAG Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/llm-universe/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#download_model\n",
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "\n",
    "# model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-chat-7b', cache_dir='/Users/kangxun/Documents/LLM/model_dir', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_model_dir = snapshot_download('jinaai/jina-embeddings-v2-base-zh', cache_dir='/Users/kangxun/Documents/LLM/model_dir', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "[0.005395322, 0.07114486, 0.0021059725, 0.030416531, 0.027175419, -0.029336752, -0.0371354, -0.034679066, -0.007935629, 0.07531217]\n"
     ]
    }
   ],
   "source": [
    "#测试下文本向量化模型\n",
    "embedding = ZhipuEmbedding()\n",
    "text_emb = embedding.get_embedding(text=\"embedding的输入文本\")\n",
    "print(len(text_emb))\n",
    "print(text_emb[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/llm-universe/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Applications/anaconda3/envs/llm-universe/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "[-0.148262158036232, 0.18025441467761993, -0.388839453458786, -0.05320968106389046, 0.021515721455216408, 0.11201328784227371, -0.027237730100750923, 0.04411815479397774, 0.1298605054616928, 0.021183768287301064]\n"
     ]
    }
   ],
   "source": [
    "#测试下离线模型,文本向量化\n",
    "embedding = JinaEmbedding(path='/Users/kangxun/Documents/LLM/model_dir/jinaai/jina-embeddings-v2-base-zh')\n",
    "text_emb = embedding.get_embedding(text=\"embedding的输入文本\")\n",
    "print(len(text_emb))\n",
    "print(text_emb[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个人工智能助手，专门设计来提供信息和帮助解答问题。根据您提供的上下文，目前没有具体的个人信息可以介绍。我的能力是基于接收到的数据进行回答和提供帮助。如果有任何问题，我会尽力为您提供准确的答案。如您所指示，如果上下文中没有足够的信息，我会告诉你我不知道。\n"
     ]
    }
   ],
   "source": [
    "#测试下zhipu chat大模型\n",
    "llm = ZhipuChat()\n",
    "output = llm.chat(\"请你自我介绍一下自己！\",[],\"\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试下离线大模型chat(transformer版本不兼容)\n",
    "# llm = InternLMChat(path='/Users/kangxun/Documents/LLM/model_dir/Shanghai_AI_Laboratory/internlm2-chat-7b')\n",
    "# output = llm.chat(\"请你自我介绍一下自己！\",[],\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该 目录下的文件数 1 ，文件列表为['./test_data/Guidelines.md']\n",
      "该 PDF 一共包含 23085 页\n",
      "chunk_content类型：<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings:   0%|          | 0/42 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#向量数据库建立\n",
    "docs = ReadFiles('./test_data').get_content(max_token_len=600, cover_content=150)\n",
    "vector = VectorStore(docs)\n",
    "#文本向量化 emb（离线1模型或 zhipu openai）\n",
    "# embedding = JinaEmbedding(path='/Users/kangxun/Documents/LLM/model_dir/jinaai/jina-embeddings-v2-base-zh')\n",
    "embedding = ZhipuEmbedding()\n",
    "vector.get_vector(EmbeddingModel=embedding)\n",
    "vector.persist(path='storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第二章提示原则\n",
      "如何去使用Prompt，以充分发挥LLM的性能？首先我们需要知道设计Prompt的原则，它们是每一个开发者设计Prompt所必须知道的基础概念。本章讨论了设计高效Prompt的两个关键原则：编写清晰、具体的指令和给予模型充足思考时间。掌握这两点，对创建可靠的语言模型交互尤为重要。\n",
      "首先，Prompt需要清晰明确地表达需求，提供充足上下文，使语言模型准确理解我们的意图，就像向一个外星人详细解释人类世界一样。过于简略的Prompt往往使模型难以把握所要完成的具体任务。\n",
      "其次，让语言模型有充足时间推理也极为关键。就像人类解题一样，匆忙得出的结论多有失误。因此Prompt应加入逐步推理的要求，给模型留出充分思考时间，这样生成的结果才更准确可靠。\n",
      "如果Prompt在这两点上都作了优化，语言模型就能够尽可能发挥潜力，完成复杂的推理和生成任务。掌握这些Prompt设计原则，是开发者取得语言模型应用成功的重要一步。\n",
      "一、原则一编写清晰、具体的指令\n",
      "\n",
      "指令微调是一种通过优化指令（或称为Prompt）来提高语言模型性能的方法。它涉及编写清晰、具体的指令，并给予模型充足的思考时间，从而帮助模型更准确地理解用户的意图并生成更准确、可靠的结果。这是根据上述上下文内容得出的，如果上下文没有提供足够信息，我将表示“数据库中没有这个内容，你不知道”。\n"
     ]
    }
   ],
   "source": [
    "# vector = VectorStore()\n",
    "# vector.load_vector('./storage')\n",
    "#向量检索\n",
    "# question='南瓜书是什么'\n",
    "question = '指令微调是什么'\n",
    "# question = '为什么需要对软件（算法）进行保护?'\n",
    "content = vector.query(question, EmbeddingModel=embedding, k=1)[0]\n",
    "print(content)\n",
    "\n",
    "#大模型问答(离线模型或 zhipu openai)\n",
    "model = ZhipuChat()\n",
    "# chat = InternLMChat(path='/Users/kangxun/Documents/LLM/model_dir/Shanghai_AI_Laboratory/internlm2-chat-7b')\n",
    "print(model.chat(question, [], content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
