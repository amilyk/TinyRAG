{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "class BaseEmbeddings:\n",
    "    \"\"\"\n",
    "    Base class for embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, path:str, is_api:bool) -> None:\n",
    "        self.path = path\n",
    "        self.is_api = is_api\n",
    "\n",
    "    def get_embedding(self, text:str, model:str) -> List[float]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @classmethod\n",
    "    def cosine_similarity(cls, vectors1:List[float], vectors2:List[float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two vectors\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(vectors1, vectors2)\n",
    "        magnitude = np.linalg.norm(vectors1) * np.linalg.norm(vectors2)\n",
    "        if not magnitude:\n",
    "            return 0\n",
    "        return dot_product / magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIEmbeddings(BaseEmbeddings):\n",
    "    \"\"\"\n",
    "    class for OpenAI embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, path:str='', is_api:bool=True) -> None:\n",
    "        super().__init__(path, is_api)\n",
    "        if self.is_api:\n",
    "            from openai import OpenAI\n",
    "            self.client = OpenAI()\n",
    "            self.client.api_key = os.getenv('OPENAI_API_KEY')\n",
    "            self.client.base_url = os.getenv('OPENAI_BASE_URL')\n",
    "    \n",
    "    def get_embedding(self, text: str, model: str = \"text-embedding-3-large\") -> List[float]:\n",
    "        if self.is_api:\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            return self.client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "        else:\n",
    "            raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class JinaEmbedding(BaseEmbeddings):\n",
    "    \"\"\"\n",
    "    class for Jina embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str = 'jinaai/jina-embeddings-v2-base-zh', is_api: bool = False) -> None:\n",
    "        super().__init__(path, is_api)\n",
    "        self._model = self.load_model()\n",
    "        \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        return self._model.encode([text])[0].tolist()\n",
    "    \n",
    "    def load_model(self):\n",
    "        import torch\n",
    "        from transformers import AutoModel\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        model = AutoModel.from_pretrained(self.path, trust_remote_code=True).to(device)\n",
    "        return model\n",
    "\n",
    "class ZhipuEmbedding(BaseEmbeddings):\n",
    "    \"\"\"\n",
    "    class for Zhipu embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, path:str='', is_api:bool=True):\n",
    "        super().__init__(path, is_api)\n",
    "        if self.is_api:\n",
    "            from zhipuai import ZhipuAI\n",
    "            self.client = ZhipuAI(api_key = os.getenv(\"ZHIPUAI_API_KEY\"))\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"embedding-2\",\n",
    "            input = text,\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "\n",
    "class DashscopeEmbedding(BaseEmbeddings):\n",
    "    \"\"\"\n",
    "    class for Dashscope embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str = '', is_api: bool = True) -> None:\n",
    "        super().__init__(path, is_api)\n",
    "        if self.is_api:\n",
    "            import dashscope\n",
    "            dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "            self.client = dashscope.TextEmbedding\n",
    "\n",
    "    def get_embedding(self, text: str, model: str='text-embedding-v1') -> List[float]:\n",
    "        response = self.client.call(\n",
    "            model=model,\n",
    "            input=text\n",
    "        )\n",
    "        return response.output['embeddings'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.文档加载及切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2\n",
    "# !pip install html2text\n",
    "# !pip install pymupdf\n",
    "# !pip install \"unstructured[md]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import markdown\n",
    "import html2text\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders.markdown import UnstructuredMarkdownLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "class ReadFiles:\n",
    "    \"\"\"\n",
    "    class to read files\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str) -> None:\n",
    "        self._path = path\n",
    "        self.file_list = self.get_files()\n",
    "\n",
    "    def get_files(self):\n",
    "        # args：dir_path，目标文件夹路径\n",
    "        file_list = []\n",
    "        for filepath, dirnames, filenames in os.walk(self._path):\n",
    "            # os.walk 函数将递归遍历指定文件夹\n",
    "            for filename in filenames:\n",
    "                # 通过后缀名判断文件类型是否满足要求\n",
    "                if filename.endswith(\".md\"):\n",
    "                    # 如果满足要求，将其绝对路径加入到结果列表\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "                elif filename.endswith(\".txt\"):\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "                elif filename.endswith(\".pdf\"):\n",
    "                    file_list.append(os.path.join(filepath, filename))\n",
    "        return file_list\n",
    "\n",
    "    def get_content(self, max_token_len: int = 600, cover_content: int = 150):\n",
    "        print(f\"该 目录下的文件数 {len(self.file_list)} ，文件列表为{self.file_list}\")\n",
    "        docs = []\n",
    "        # 读取文件内容\n",
    "        for file in self.file_list:\n",
    "            content = self.read_file_content(file)\n",
    "            \n",
    "            chunk_content = self.get_chunk(\n",
    "                content, max_token_len=max_token_len, cover_content=cover_content)\n",
    "            # print(f'chunk_content：{chunk_content}')\n",
    "            docs.extend(chunk_content)\n",
    "            print(f\"该 PDF 一共包含 {len(content)} 字, chunk数目{len(chunk_content)}\")\n",
    "            # print(f'chunk长度{len(chunk_content)},加上该文件后的长度{len(docs)},docs:{docs}')\n",
    "            # docs.append(chunk_content)\n",
    "        return docs\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def read_file_content(cls, file_path: str):\n",
    "        # 根据文件扩展名选择读取方法\n",
    "        if file_path.endswith('.pdf'):\n",
    "            return cls.read_pdf(file_path)\n",
    "        elif file_path.endswith('.md'):\n",
    "            return cls.read_markdown(file_path)\n",
    "        elif file_path.endswith('.txt'):\n",
    "            return cls.read_text(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    # @classmethod\n",
    "    # def read_pdf(cls, file_path: str):\n",
    "    #     # 读取PDF文件\n",
    "    #     with open(file_path, 'rb') as file:\n",
    "    #         reader = PyPDF2.PdfReader(file)\n",
    "    #         text = \"\"\n",
    "    #         for page_num in range(len(reader.pages)):\n",
    "    #             text += reader.pages[page_num].extract_text()\n",
    "    #         return text\n",
    "    @classmethod\n",
    "    def read_pdf(cls, file_path: str):\n",
    "        # 读取PDF文件\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "        pdf_pages = loader.load()\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_pages)):\n",
    "            pdf_page = pdf_pages[page_num]\n",
    "            # 处理每一页的文本\n",
    "            import re\n",
    "            pattern = re.compile(r'[^\\u4e00-\\u9fff](\\n)[^\\u4e00-\\u9fff]', re.DOTALL)\n",
    "            pdf_page.page_content = re.sub(pattern, lambda match: match.group(0).replace('\\n', ''), pdf_page.page_content)\n",
    "            pdf_page.page_content = pdf_page.page_content.replace('•', '')\n",
    "            pdf_page.page_content = pdf_page.page_content.replace(' ', '')\n",
    "            text += pdf_page.page_content\n",
    "        return text\n",
    "    \n",
    "\n",
    "\n",
    "    # @classmethod\n",
    "    # def read_markdown(cls, file_path: str):\n",
    "    #     # 读取Markdown文件\n",
    "    #     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    #         md_text = file.read()\n",
    "    #         html_text = markdown.markdown(md_text)\n",
    "    #         # 使用BeautifulSoup从HTML中提取纯文本\n",
    "    #         soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    #         plain_text = soup.get_text()\n",
    "    #         # 使用正则表达式移除网址链接\n",
    "    #         text = re.sub(r'http\\S+', '', plain_text) \n",
    "    #         return text\n",
    "    \n",
    "    @classmethod\n",
    "    def read_markdown(cls, file_path: str):\n",
    "        # 读取Markdown文件\n",
    "        loader = UnstructuredMarkdownLoader(file_path)\n",
    "        md_pages = loader.load()\n",
    "        text = \"\"\n",
    "        for page_num in range(len(md_pages)):\n",
    "            md_page = md_pages[page_num]\n",
    "            md_page.page_content = md_page.page_content.replace('\\n\\n', '\\n')\n",
    "            text += md_page.page_content\n",
    "        return text\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def read_text(cls, file_path: str):\n",
    "        # 读取文本文件\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    \n",
    "    @classmethod\n",
    "    def get_chunk(cls, text: str, max_token_len: int = 600, cover_content: int = 150):\n",
    "        chunk_text = []\n",
    "\n",
    "        curr_len = 0\n",
    "        curr_chunk = ''\n",
    "\n",
    "        token_len = max_token_len - cover_content\n",
    "        lines = text.splitlines()  # 假设以换行符分割文本为行\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.replace(' ', '')\n",
    "            line_len = len(enc.encode(line))\n",
    "            if line_len > max_token_len:\n",
    "                # 如果单行长度就超过限制，则将其分割成多个块\n",
    "                num_chunks = (line_len + token_len - 1) // token_len\n",
    "                for i in range(num_chunks):\n",
    "                    start = i * token_len\n",
    "                    end = start + token_len\n",
    "                    # 避免跨单词分割\n",
    "                    while not line[start:end].rstrip().isspace():\n",
    "                        start += 1\n",
    "                        end += 1\n",
    "                        if start >= line_len:\n",
    "                            break\n",
    "                    curr_chunk = curr_chunk[-cover_content:] + line[start:end]\n",
    "                    chunk_text.append(curr_chunk)\n",
    "                # 处理最后一个块\n",
    "                start = (num_chunks - 1) * token_len\n",
    "                curr_chunk = curr_chunk[-cover_content:] + line[start:end]\n",
    "                chunk_text.append(curr_chunk)\n",
    "                \n",
    "            if curr_len + line_len <= token_len:\n",
    "                curr_chunk += line\n",
    "                curr_chunk += '\\n'\n",
    "                curr_len += line_len\n",
    "                curr_len += 1\n",
    "            else:\n",
    "                chunk_text.append(curr_chunk)\n",
    "                curr_chunk = curr_chunk[-cover_content:]+line\n",
    "                curr_len = line_len + cover_content\n",
    "\n",
    "        if curr_chunk:\n",
    "            chunk_text.append(curr_chunk)\n",
    "\n",
    "        return chunk_text\n",
    "\n",
    "\n",
    "class Documents:\n",
    "    \"\"\"\n",
    "        获取已分好类的json格式文档\n",
    "    \"\"\"\n",
    "    def __init__(self, path: str = '') -> None:\n",
    "        self.path = path\n",
    "    \n",
    "    def get_content(self):\n",
    "        with open(self.path, mode='r', encoding='utf-8') as f:\n",
    "            content = json.load(f)\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试文件处理\n",
    "\n",
    "# readfiles = ReadFiles('./data')\n",
    "# content = readfiles.read_file_content(readfiles.file_list[0])\n",
    "# docs= readfiles.get_content(content, max_token_len=100, cover_content=20)\n",
    "# docs= readfiles.get_content(max_token_len=600, cover_content=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.数据库 && 向量检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict,List, Optional, Tuple, Union\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, document:List[str]=['']) -> None:\n",
    "        self.document = document\n",
    "        self.vectors = []\n",
    "    \n",
    "    def get_vector(self, EmbeddingModel: BaseEmbeddings) -> List[List[float]]:\n",
    "        # 获得文档的向量表示\n",
    "        for doc in tqdm(self.document, desc=\"Calculating embeddings\"):\n",
    "            # print(f'emb : {EmbeddingModel.get_embedding(doc)}')\n",
    "            self.vectors.append(EmbeddingModel.get_embedding(doc))\n",
    "        return self.vectors\n",
    "\n",
    "    def persist(self, path: str = 'storage'):\n",
    "        # 数据库持久化，本地保存\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        with open(f\"{path}/document.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.document, f, ensure_ascii=False)\n",
    "        if self.vectors:\n",
    "            with open(f\"{path}/vectors.json\", 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.vectors, f)\n",
    "\n",
    "    def load_vector(self, path: str = 'storage'):\n",
    "        with open(f\"{path}/vectors.json\", 'r', encoding='utf-8') as f:\n",
    "            self.vectors = json.load(f)\n",
    "        with open(f\"{path}/document.json\", 'r', encoding='utf-8') as f:\n",
    "            self.document = json.load(f)\n",
    "\n",
    "    def query(self, query: str, EmbeddingModel: BaseEmbeddings, k: int = 1) -> List[str]:\n",
    "        # 根据问题检索相关的文档片段\n",
    "        # print(f'doc:{self.document[:5]}')\n",
    "        query_vector = EmbeddingModel.get_embedding(query)\n",
    "        # print(f'query_vector:{query_vector[:3]}, len_vector:{len(self.vectors)}')\n",
    "        result = np.array([self.get_similarity(query_vector, vector) for vector in self.vectors])\n",
    "        # print(f'result:{result}')\n",
    "        return np.array(self.document)[result.argsort()[-k:][::-1]].tolist()\n",
    "\n",
    "    def get_similarity(self,vector1:List[float], vector2:List[float]) -> float:\n",
    "        return BaseEmbeddings.cosine_similarity(vector1, vector2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.大模型模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = dict(\n",
    "    RAG_PROMPT_TEMPALTE=\"\"\"使用以上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。\n",
    "        问题: {question}\n",
    "        可参考的上下文：\n",
    "        ···\n",
    "        {context}\n",
    "        ···\n",
    "        如果给定的上下文无法让你做出回答，请回答数据库中没有这个内容，你不知道。\n",
    "        有用的回答:\"\"\",\n",
    "    InternLM_PROMPT_TEMPALTE=\"\"\"先对上下文进行内容总结,再使用上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。\n",
    "        问题: {question}\n",
    "        可参考的上下文：\n",
    "        ···\n",
    "        {context}\n",
    "        ···\n",
    "        如果给定的上下文无法让你做出回答，请回答数据库中没有这个内容，你不知道。\n",
    "        有用的回答:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def __init__(self, path: str = '') -> None:\n",
    "        self.path = path\n",
    "\n",
    "    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n",
    "        pass\n",
    "\n",
    "    def load_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIChat(BaseModel):\n",
    "    def __init__(self, path: str = '', model: str = \"gpt-3.5-turbo-1106\") -> None:\n",
    "        super().__init__(path)\n",
    "        self.model = model\n",
    "\n",
    "    def chat(self, prompt: str, history: List[dict], content: str) -> str:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        client.api_key = os.getenv(\"OPENAI_API_KEY\")   \n",
    "        client.base_url = os.getenv(\"OPENAI_BASE_URL\")\n",
    "        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=history,\n",
    "            max_tokens=150,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternLMChat(BaseModel):\n",
    "    def __init__(self, path: str = '') -> None:\n",
    "        super().__init__(path)\n",
    "        self.load_model()\n",
    "\n",
    "    def chat(self, prompt: str, history: List = [], content: str='') -> str:\n",
    "        prompt = PROMPT_TEMPLATE['InternLM_PROMPT_TEMPALTE'].format(question=prompt, context=content)\n",
    "        response, history = self.model.chat(self.tokenizer, prompt, history)\n",
    "        return response\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        import torch\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.path, trust_remote_code=True)\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.path, torch_dtype=torch.float16, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DashscopeChat(BaseModel):\n",
    "    def __init__(self, path: str = '', model: str = \"qwen-turbo\") -> None:\n",
    "        super().__init__(path)\n",
    "        self.model = model\n",
    "\n",
    "    def chat(self, prompt: str, history: List[Dict], content: str) -> str:\n",
    "        import dashscope\n",
    "        dashscope.api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n",
    "        response = dashscope.Generation.call(\n",
    "            model=self.model,\n",
    "            messages=history,\n",
    "            result_format='message',\n",
    "            max_tokens=150,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.output.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZhipuChat(BaseModel):\n",
    "    def __init__(self, path: str = '', model: str = \"glm-4\") -> None:\n",
    "        super().__init__(path)\n",
    "        from zhipuai import ZhipuAI\n",
    "        self.client = ZhipuAI(api_key=os.getenv(\"ZHIPUAI_API_KEY\"))\n",
    "        self.model = model\n",
    "\n",
    "    def chat(self, prompt: str, history: List[Dict], content: str) -> str:\n",
    "        history.append({'role': 'user', 'content': PROMPT_TEMPLATE['RAG_PROMPT_TEMPALTE'].format(question=prompt, context=content)})\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=history,\n",
    "            max_tokens=150,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Tiny-RAG Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download_model\n",
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "\n",
    "# model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2-chat-7b', cache_dir='/Users/kangxun/Documents/LLM/model_dir', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_model_dir = snapshot_download('jinaai/jina-embeddings-v2-base-zh', cache_dir='/Users/kangxun/Documents/LLM/model_dir', revision='master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "[0.005395322, 0.07114486, 0.0021059725, 0.030416531, 0.027175419, -0.029336752, -0.0371354, -0.034679066, -0.007935629, 0.07531217]\n"
     ]
    }
   ],
   "source": [
    "#测试下文本向量化模型\n",
    "embedding = ZhipuEmbedding()\n",
    "text_emb = embedding.get_embedding(text=\"embedding的输入文本\")\n",
    "print(len(text_emb))\n",
    "print(text_emb[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "[-0.148262158036232, 0.18025441467761993, -0.388839453458786, -0.05320968106389046, 0.021515721455216408, 0.11201328784227371, -0.027237730100750923, 0.04411815479397774, 0.1298605054616928, 0.021183768287301064]\n"
     ]
    }
   ],
   "source": [
    "#测试下离线模型,文本向量化\n",
    "embedding = JinaEmbedding(path='/Users/kangxun/Documents/LLM/model_dir/jinaai/jina-embeddings-v2-base-zh')\n",
    "text_emb = embedding.get_embedding(text=\"embedding的输入文本\")\n",
    "print(len(text_emb))\n",
    "print(text_emb[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个人工智能助手，专门设计来提供信息和帮助解答问题。根据您提供的上下文，目前没有具体的个人信息可以介绍。如果您有其他问题或需要帮助，请随时告诉我。\n"
     ]
    }
   ],
   "source": [
    "#测试下zhipu chat大模型\n",
    "llm = ZhipuChat()\n",
    "output = llm.chat(\"请你自我介绍一下自己！\",[],\"\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试下离线大模型chat(transformer版本不兼容)\n",
    "# llm = InternLMChat(path='/Users/kangxun/Documents/LLM/model_dir/Shanghai_AI_Laboratory/internlm2-chat-7b')\n",
    "# output = llm.chat(\"请你自我介绍一下自己！\",[],\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该 目录下的文件数 4 ，文件列表为['./data/软件专利申请及权利保护.md', './data/pumpkin_book.pdf', './data/Guidelines.md', './data/Introduction.md']\n",
      "该 PDF 一共包含 7698 字, chunk数目32\n",
      "该 PDF 一共包含 242230 字, chunk数目752\n",
      "该 PDF 一共包含 23085 字, chunk数目42\n",
      "该 PDF 一共包含 1782 字, chunk数目7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 833/833 [01:19<00:00, 10.43it/s]\n"
     ]
    }
   ],
   "source": [
    "#向量数据库建立\n",
    "docs = ReadFiles('./data').get_content(max_token_len=600, cover_content=150)\n",
    "# print(f'docs:{len(docs)}')\n",
    "vector = VectorStore(docs)\n",
    "#文本向量化 emb（离线1模型或 zhipu openai）\n",
    "# embedding = JinaEmbedding(path='/Users/kangxun/Documents/LLM/model_dir/jinaai/jina-embeddings-v2-base-zh')\n",
    "embedding = ZhipuEmbedding()\n",
    "\n",
    "vector.get_vector(EmbeddingModel=embedding)\n",
    "# print(len(vector.vectors))\n",
    "vector.persist(path='storage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['恰当的保护除了可以有效地降低自己在未来遭遇诉讼的风险，也让自己可以在未来的“被侵权”中可以提供更多的证据来有效夺回自己的利益，目前在华为也已经出现了售卖专利使用权的方式来获取远超开发乃至申请专利所消耗的资源的盈利——利用每年几万件的专利申请，将自己的知识产权和自己的开发成果牢牢保护在自己的专利墙中。在当下，腾讯等国内领先的科技公司都不再停留在简单的软著保护，而是开始对软件的执行方法乃至算法进行保护，从而将自己的技术紧紧保护在自己的专利墙中。在法律愈来愈完善的情况下，专利的申请与保护也愈来愈重要。19.1.1如果不保护自己的软件（算法）会怎样？\\n', '[TOC]\\n第十九章软件（算法）专利申请及权利保护\\nMarkdownRevision1;\\nDate:2019/07/16\\nEditor:何建宏\\nContact:bonopengate@gmail.com\\n19.1为什么需要对软件（算法）进行保护？\\n对软件/系统/算法进行保护可以有效地保护在计算机领域中的公司或个人的权益，随着人工智能的兴起，在图像处理、语音处理、文本处理等方向上，公司或个人不断地研发新的系统，探究新的算法，可是随着软件的开发/设计成本逐渐增高，愈来愈多公司或个人开始对对手产品进行模仿，而这个过程中，被模仿的公司或个人也是深受其害。在美国早已有专门设立的对软件的专利保护政策，而国内因为各种因素迟迟未有这方面的实行政策。所以目前软件开发者或者算法设计者只能通过其它的方法来保护自己的权益，保护自己的知识产权。【1】\\n', '知识产权和自己的开发成果牢牢保护在自己的专利墙中。在当下，腾讯等国内领先的科技公司都不再停留在简单的软著保护，而是开始对软件的执行方法乃至算法进行保护，从而将自己的技术紧紧保护在自己的专利墙中。在法律愈来愈完善的情况下，专利的申请与保护也愈来愈重要。19.1.1如果不保护自己的软件（算法）会怎样？\\n在《中国法院知识产权司法保护状况（2018）》中指出，2018年，人民法院共新收一审、二审、申请再审等各类知识产权案件334951件，审结319651件（含旧存），比2017年分别上升41.19%和41.64%。其中，竞争类一审案件数量(含垄断民事案件)增幅最为显著,同比上升63.04%,达到4146件。其中的新收专利案件为21699，同比上升35.53%。而其中有一个特别的就是百度诉搜狗侵权的案例，判决书为北京市高级人民法院（2018）京民终498号民事判决书，诉讼的内容就是输入法的操作本身，这个和申诉算法或者软件相关专利其实已经十分地类似了，如果搜狗败诉，赔偿额则是一千万，但是因为搜狗本身拥有自己的输入法操作的发明专利，因此百度与它的专利纠纷最后以百度败诉收场。']\n"
     ]
    }
   ],
   "source": [
    "# vector = VectorStore()\n",
    "# vector.load_vector('./storage')\n",
    "#向量检索\n",
    "question='如果不保护自己的软件（算法）会怎样'\n",
    "# question = '软件（算法）专利申请及权利保护是谁写的？'\n",
    "# question = '南瓜书是谁写的？'\n",
    "content = vector.query(question, EmbeddingModel=embedding, k=3)\n",
    "print(content)\n",
    "\n",
    "#大模型问答(离线模型或 zhipu openai)\n",
    "# model = ZhipuChat()\n",
    "# model = OpenAIChat()\n",
    "# chat = InternLMChat(path='/Users/kangxun/Documents/LLM/model_dir/Shanghai_AI_Laboratory/internlm2-chat-7b')\n",
    "# print(model.chat(question, [], content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果不保护自己的软件（算法），可能会面临以下风险和后果：\n",
      "\n",
      "1. **容易遭受侵权**：没有保护的软件或算法容易被竞争对手模仿或直接使用，导致开发者的权益受损。\n",
      "\n",
      "2. **难以提供法律证据**：在发生侵权行为时，如果没有相应的专利或著作权等法律保护，开发者将难以提供有效证据来维护自己的权益。\n",
      "\n",
      "3. **经济损失**：侵权行为可能导致开发者失去潜在的市场份额和收益，同时如果侵权方因此获利，开发者难以追讨相应的经济损失。\n",
      "\n",
      "4. **法律诉讼风险**：如果软件或算法被他人申请了专利保护，开发者可能面临被诉侵权的风险，这可能导致巨额的赔偿。\n",
      "\n",
      "5. **技术优势丧失**：软件和\n"
     ]
    }
   ],
   "source": [
    "model = ZhipuChat()\n",
    "print(model.chat(question, [], content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
